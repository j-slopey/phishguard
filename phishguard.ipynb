{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319bcc8e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "079b93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nigerian_Fraud.csv\n",
      "Ling.csv\n",
      "Nazario.csv\n",
      "SpamAssasin.csv\n",
      "CEAS_08.csv\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, loguniform\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"naserabdullahalam/phishing-email-dataset\")\n",
    "data = []\n",
    "\n",
    "datasets = [\n",
    "    \"Nigerian_Fraud.csv\",\n",
    "    \"Ling.csv\",\n",
    "    \"Nazario.csv\",\n",
    "    \"SpamAssasin.csv\",\n",
    "    \"CEAS_08.csv\",\n",
    "    # \"Enron.csv\"\n",
    "]\n",
    "for file in datasets:\n",
    "    print(file)\n",
    "    csv_path = os.path.join(path, file)\n",
    "    subset_data = pd.read_csv(csv_path)\n",
    "    data.append(subset_data)\n",
    "\n",
    "all_data = pd.concat(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f54594",
   "metadata": {},
   "source": [
    "## Clean & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daf6a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['receiver'] = all_data['receiver'].str.replace('undisclosed-recipients:;', 'Unknown')\n",
    "# -- Timestamp features -- \n",
    "all_data['date_parsed'] = pd.to_datetime(all_data['date'], errors='coerce', utc=True)\n",
    "\n",
    "# Week-of-year\n",
    "iso_week = all_data['date_parsed'].dt.isocalendar().week\n",
    "iso_week = iso_week.astype(float)\n",
    "week0 = ((iso_week - 1) % 52)\n",
    "theta_week = 2.0 * np.pi * week0 / 52\n",
    "all_data['sin_week'] = np.where(week0.notna(), np.sin(theta_week), 0.0)\n",
    "all_data['cos_week'] = np.where(week0.notna(), np.cos(theta_week), 0.0)\n",
    "\n",
    "# Hour-of-day \n",
    "hour = all_data['date_parsed'].dt.hour.astype(float)  # NaN for missing\n",
    "theta_hour = 2.0 * np.pi * hour / 24\n",
    "all_data['sin_hour'] = np.where(hour.notna(), np.sin(theta_hour), 0.0)\n",
    "all_data['cos_hour'] = np.where(hour.notna(), np.cos(theta_hour), 0.0)\n",
    "\n",
    "# Weekend binary (0/1)\n",
    "weekday = all_data['date_parsed'].dt.weekday\n",
    "all_data['is_weekend'] = np.where(weekday.isna(), 0, ((weekday >= 5).astype(int)))\n",
    "\n",
    "# Timestamp feature list\n",
    "timestamp_features = [\n",
    "    \"sin_week\",\n",
    "    \"cos_week\",\n",
    "    \"sin_hour\",\n",
    "    \"cos_hour\",\n",
    "    \"is_weekend\"\n",
    "]\n",
    "\n",
    "# -- Sender/reciever feature engineering -- \n",
    "with open('domains.json', 'r') as file:\n",
    "    public_email_domains = json.load(file)\n",
    "    \n",
    "email_regex = r'([a-zA-Z0-9._%+\\-|{}^&\"\\'=]+@(?:[a-zA-Z0-9.-]+|\\[[0-9.]+\\]))'    \n",
    "for column_name in ('sender', 'receiver'):\n",
    "    all_data[f'{column_name}_email'] = all_data[column_name].str.extract(email_regex, expand=False)\n",
    "    all_data[f'{column_name}_domain'] = all_data[f'{column_name}_email'].str.split('@', n=1).str[1]\n",
    "    all_data[f'{column_name}_domain_len'] = all_data[f'{column_name}_domain'].str.len()\n",
    "    all_data[f'{column_name}_domain_public'] = all_data[f'{column_name}_domain'].str.lower().isin(public_email_domains).astype(int)\n",
    "    all_data[f'{column_name}_n_subdomains'] = all_data[f'{column_name}_domain'].str.lower().str.count(r'\\.')\n",
    "    all_data[f'{column_name}_email_n_digits'] = all_data[f'{column_name}_domain'].str.lower().str.count(r'\\d')\n",
    "    \n",
    "    all_data[f'{column_name}_name'] = all_data[column_name].str.replace(email_regex, '', regex=True)\n",
    "    all_data[f'{column_name}_name'] = all_data[f'{column_name}_name'].str.replace(r'[<>\"\\'\\(\\)]', '', regex=True).str.strip()\n",
    "    \n",
    "all_data['is_internal_email'] = (\n",
    "    (all_data['sender_domain'] == all_data['receiver_domain']) & \n",
    "    (all_data['sender_domain'].notna())\n",
    ").astype(int)\n",
    "\n",
    "all_data['sender_name_contains_email'] = all_data['sender_name'].str.contains('@', na=False).astype(int)\n",
    "\n",
    "# Sender/reciever feature list\n",
    "email_features = [\n",
    "    \"sender_domain_public\",\n",
    "    \"sender_domain_len\",\n",
    "    \"sender_n_subdomains\",\n",
    "    \"sender_email_n_digits\",\n",
    "    \"sender_name_contains_email\",\n",
    "    # \"is_internal_email\"\n",
    "]\n",
    "\n",
    "# -- Fill in url count for missing entries -- \n",
    "url_regex = r'((?:https?|ftp)://\\S+|www\\.\\S+)'\n",
    "text_column = 'body' \n",
    "missing_count_mask = all_data['urls'].isna()\n",
    "all_data.loc[missing_count_mask, 'urls'] = (\n",
    "    all_data.loc[missing_count_mask, text_column]\n",
    "    .astype(str)\n",
    "    .str.count(url_regex)\n",
    ")\n",
    "\n",
    "all_data[['body', 'subject']] = all_data[['body', 'subject']].fillna('Unknown')\n",
    "\n",
    "feature_set= [\n",
    "    'subject',\n",
    "    'body',\n",
    "    *email_features,\n",
    "    *timestamp_features\n",
    "]\n",
    "\n",
    "X = all_data[feature_set]\n",
    "y = all_data['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66bc3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Search...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "# -- Initialize Model --\n",
    "vectorizer = ColumnTransformer([\n",
    "    ('subject_word_vectorizer', TfidfVectorizer(lowercase=False, analyzer='word'), 'subject'),\n",
    "    ('subject_charwb_vectorizer', TfidfVectorizer(lowercase=False, analyzer='char_wb'), 'subject'),\n",
    "    ('body_vectorizer', TfidfVectorizer(lowercase=True, analyzer='word'), 'body' )\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('text_vectorizer', vectorizer),\n",
    "    ('xgboost', XGBClassifier())\n",
    "])\n",
    "\n",
    "# -- Hyperparameter Tuning --\n",
    "param_dist = {\n",
    "    'xgboost__n_estimators': range(100, 500),\n",
    "    'xgboost__learning_rate': loguniform(0.01, 0.3),\n",
    "    'xgboost__max_depth': range(3, 10),\n",
    "    'xgboost__subsample': uniform(0.6, 0.1), \n",
    "    \n",
    "    'text_vectorizer__body_vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'text_vectorizer__body_vectorizer__min_df': loguniform(1e-4, 0.1),\n",
    "    \n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    model_pipeline, \n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Starting Random Search...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Score: {random_search.best_score_}\")\n",
    "print(f\"Best Params: {random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c672d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99267   0.98397   0.98830      4678\n",
      "           1    0.98730   0.99420   0.99074      5866\n",
      "\n",
      "    accuracy                        0.98966     10544\n",
      "   macro avg    0.98999   0.98909   0.98952     10544\n",
      "weighted avg    0.98968   0.98966   0.98966     10544\n",
      "\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFittedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m xg_model = model_pipeline.named_steps[\u001b[33m'\u001b[39m\u001b[33mxgboost\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     11\u001b[39m preprocessor = model_pipeline.named_steps[\u001b[33m'\u001b[39m\u001b[33mtext_vectorizer\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m feature_names = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m importances = xg_model.feature_importances_\n\u001b[32m     16\u001b[39m feats_df = pd.DataFrame({\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mFeature\u001b[39m\u001b[33m'\u001b[39m: feature_names,\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mImportance\u001b[39m\u001b[33m'\u001b[39m: importances\n\u001b[32m     19\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/phishguard/.venv/lib/python3.13/site-packages/sklearn/compose/_column_transformer.py:622\u001b[39m, in \u001b[36mColumnTransformer.get_feature_names_out\u001b[39m\u001b[34m(self, input_features)\u001b[39m\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_feature_names_out\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_features=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get output feature names for transformation.\u001b[39;00m\n\u001b[32m    604\u001b[39m \n\u001b[32m    605\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    620\u001b[39m \u001b[33;03m        Transformed feature names.\u001b[39;00m\n\u001b[32m    621\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m     input_features = _check_feature_names_in(\u001b[38;5;28mself\u001b[39m, input_features)\n\u001b[32m    625\u001b[39m     \u001b[38;5;66;03m# List of tuples (name, feature_names_out)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/phishguard/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1754\u001b[39m, in \u001b[36mcheck_is_fitted\u001b[39m\u001b[34m(estimator, attributes, msg, all_or_any)\u001b[39m\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg % {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator).\u001b[34m__name__\u001b[39m})\n",
      "\u001b[31mNotFittedError\u001b[39m: This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# -- Analyze results --\n",
    "\n",
    "y_pred = random_search.predict(X_test)\n",
    "model_result = classification_report(y_test, y_pred, digits=5)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(model_result)\n",
    "\n",
    "\n",
    "xg_model = model_pipeline.named_steps['xgboost']\n",
    "preprocessor = model_pipeline.named_steps['text_vectorizer']\n",
    "\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "importances = xg_model.feature_importances_\n",
    "\n",
    "feats_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "print(feats_df.sort_values(by='Importance', ascending=False).head(30))\n",
    "with open('results.txt', 'a') as file:\n",
    "    file.write(f\"\"\"\n",
    "\\n--- RESULTS ---\n",
    "Datasets Used: {datasets}\n",
    "Processing: {preprocessor.named_transformers_}\n",
    "Features: {X.columns.tolist()}\n",
    "{model_result}\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
