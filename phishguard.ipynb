{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319bcc8e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079b93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\James\\Documents\\phishguard\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nigerian_Fraud.csv\n",
      "Ling.csv\n",
      "Nazario.csv\n",
      "SpamAssasin.csv\n",
      "CEAS_08.csv\n"
     ]
    }
   ],
   "source": [
    "import os, time, json\n",
    "import kagglehub\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"naserabdullahalam/phishing-email-dataset\")\n",
    "data = []\n",
    "\n",
    "datasets = [\n",
    "    \"Nigerian_Fraud.csv\",\n",
    "    \"Ling.csv\",\n",
    "    \"Nazario.csv\",\n",
    "    \"SpamAssasin.csv\",\n",
    "    \"CEAS_08.csv\",\n",
    "    # \"Enron.csv\"\n",
    " ]\n",
    "for file in datasets:\n",
    "    print(file)\n",
    "    csv_path = os.path.join(path, file)\n",
    "    subset_data = pd.read_csv(csv_path)\n",
    "    data.append(subset_data)\n",
    "\n",
    "all_data = pd.concat(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f54594",
   "metadata": {},
   "source": [
    "## Clean & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf6a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['receiver'] = all_data['receiver'].str.replace('undisclosed-recipients:;', 'Unknown')\n",
    "# -- Timestamp features -- \n",
    "all_data['date_parsed'] = pd.to_datetime(all_data['date'], errors='coerce', utc=True)\n",
    "\n",
    "# Week-of-year\n",
    "iso_week = all_data['date_parsed'].dt.isocalendar().week\n",
    "iso_week = iso_week.astype(float)\n",
    "week0 = ((iso_week - 1) % 52)\n",
    "theta_week = 2.0 * np.pi * week0 / 52\n",
    "all_data['sin_week'] = np.where(week0.notna(), np.sin(theta_week), 0.0)\n",
    "all_data['cos_week'] = np.where(week0.notna(), np.cos(theta_week), 0.0)\n",
    "\n",
    "# Hour-of-day \n",
    "hour = all_data['date_parsed'].dt.hour.astype(float)  # NaN for missing\n",
    "theta_hour = 2.0 * np.pi * hour / 24\n",
    "all_data['sin_hour'] = np.where(hour.notna(), np.sin(theta_hour), 0.0)\n",
    "all_data['cos_hour'] = np.where(hour.notna(), np.cos(theta_hour), 0.0)\n",
    "\n",
    "# Weekend binary (0/1)\n",
    "weekday = all_data['date_parsed'].dt.weekday\n",
    "all_data['is_weekend'] = np.where(weekday.isna(), 0, ((weekday >= 5).astype(int)))\n",
    "\n",
    "# Timestamp feature list\n",
    "timestamp_features = [\n",
    "    \"sin_week\",\n",
    "    \"cos_week\",\n",
    "    \"sin_hour\",\n",
    "    \"cos_hour\",\n",
    "    \"is_weekend\"\n",
    "]\n",
    "\n",
    "# -- Sender/reciever feature engineering -- \n",
    "with open('domains.json', 'r') as file:\n",
    "    public_email_domains = json.load(file)\n",
    "    \n",
    "email_regex = r'([a-zA-Z0-9._%+\\-|{}^&\"\\'=]+@(?:[a-zA-Z0-9.-]+|\\[[0-9.]+\\]))'    \n",
    "for column_name in ('sender', 'receiver'):\n",
    "    all_data[f'{column_name}_email'] = all_data[column_name].str.extract(email_regex, expand=False)\n",
    "    all_data[f'{column_name}_domain'] = all_data[f'{column_name}_email'].str.split('@', n=1).str[1]\n",
    "    all_data[f'{column_name}_domain_len'] = all_data[f'{column_name}_domain'].str.len()\n",
    "    all_data[f'{column_name}_domain_public'] = all_data[f'{column_name}_domain'].str.lower().isin(public_email_domains).astype(int)\n",
    "    all_data[f'{column_name}_n_subdomains'] = all_data[f'{column_name}_domain'].str.lower().str.count(r'\\.')\n",
    "    all_data[f'{column_name}_email_n_digits'] = all_data[f'{column_name}_domain'].str.lower().str.count(r'\\d')\n",
    "    \n",
    "    all_data[f'{column_name}_name'] = all_data[column_name].str.replace(email_regex, '', regex=True)\n",
    "    all_data[f'{column_name}_name'] = all_data[f'{column_name}_name'].str.replace(r'[<>\"\\'\\(\\)]', '', regex=True).str.strip()\n",
    "    \n",
    "all_data['is_internal_email'] = (\n",
    "    (all_data['sender_domain'] == all_data['receiver_domain']) & \n",
    "    (all_data['sender_domain'].notna())\n",
    ").astype(int)\n",
    "\n",
    "all_data['sender_name_contains_email'] = all_data['sender_name'].str.contains('@', na=False).astype(int)\n",
    "\n",
    "# Sender/reciever feature list\n",
    "email_features = [\n",
    "    \"sender_domain_public\",\n",
    "    \"sender_domain_len\",\n",
    "    \"sender_n_subdomains\",\n",
    "    \"sender_email_n_digits\",\n",
    "    \"sender_name_contains_email\",\n",
    "    # \"is_internal_email\"\n",
    "]\n",
    "\n",
    "# -- Fill in url count for missing entries -- \n",
    "url_regex = r'((?:https?|ftp)://\\S+|www\\.\\S+)'\n",
    "text_column = 'body' \n",
    "missing_count_mask = all_data['urls'].isna()\n",
    "all_data.loc[missing_count_mask, 'urls'] = (\n",
    "    all_data.loc[missing_count_mask, text_column]\n",
    "    .astype(str)\n",
    "    .str.count(url_regex)\n",
    ")\n",
    "\n",
    "all_data[['body', 'subject']] = all_data[['body', 'subject']].fillna('Unknown')\n",
    "\n",
    "feature_set= [\n",
    "    'subject',\n",
    "    'body',\n",
    "    *email_features,\n",
    "    *timestamp_features\n",
    "]\n",
    "\n",
    "X = all_data[feature_set]\n",
    "y = all_data['label'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66bc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Initialize Model --\n",
    "vectorizer = ColumnTransformer([\n",
    "    ('subject_word', TfidfVectorizer(lowercase=False, analyzer='word'), 'subject'),\n",
    "    ('subject_charwb', TfidfVectorizer(lowercase=False, analyzer='char_wb'), 'subject'),\n",
    "    ('body', TfidfVectorizer(lowercase=True, analyzer='word'), 'body' )\n",
    "    ],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('xgboost', XGBClassifier())\n",
    "        \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c672d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 15:43:00,600] A new study created in memory with name: no-name-9c61bac6-c6e3-4d71-a854-8a3fa51dcb5d\n",
      "c:\\Users\\James\\Documents\\phishguard\\.venv\\Lib\\site-packages\\xgboost\\core.py:774: UserWarning: [15:43:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n",
      "[I 2025-12-10 15:43:17,404] Trial 0 finished with value: 0.9892799133730373 and parameters: {'n_estimators': 824, 'learning_rate': 0.29860806279021457, 'max_depth': 7, 'subsample': 0.33738070465253533, 'colsample_bytree': 0.996417707625955, 'ngram_range': 'unigram', 'stop_words_setting': 'english', 'df_min_type': 'float', 'min_df': 0.001329279625747657, 'max_features': 23666}. Best is trial 0 with value: 0.9892799133730373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 done in 16.80s | F1: 0.9893\n"
     ]
    }
   ],
   "source": [
    "# -- Hyperparameter tuning with Optuna -- \n",
    "GPU = True\n",
    "def objective(trial):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define Search Space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 12)\n",
    "    subsample = trial.suggest_float('subsample', 0.15, .9)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    \n",
    "    ngram_choice = trial.suggest_categorical('ngram_range', ['unigram', 'bigram'])\n",
    "    ngram_range = (1, 1) if ngram_choice == 'unigram' else (1, 2)\n",
    "    stop_words_choice = trial.suggest_categorical('stop_words_setting', ['english', 'none'])\n",
    "    stop_words = 'english' if stop_words_choice == 'english' else None\n",
    "    df_min_type = trial.suggest_categorical('df_min_type', ['int', 'float'])\n",
    "\n",
    "\n",
    "    max_features = None\n",
    "    if df_min_type == 'int':\n",
    "        min_df = trial.suggest_int('df_min', 2, 20)\n",
    "    elif df_min_type == 'float':\n",
    "        min_df = trial.suggest_float('min_df', 0.0001, 0.3, log=True)\n",
    "\n",
    "    \n",
    "    max_features = trial.suggest_int('max_features', 10000, 200000) if min_df < 0.01 or min_df == 2 else None\n",
    "\n",
    "    \n",
    "    # Build Vectorizer\n",
    "    vectorizer = ColumnTransformer([\n",
    "        ('subject_word_vectorizer', TfidfVectorizer(lowercase=False, analyzer='word', ngram_range=ngram_range), 'subject'),\n",
    "        ('subject_charwb_vectorizer', TfidfVectorizer(lowercase=False, analyzer='char_wb'), 'subject'),\n",
    "        ('body_vectorizer', TfidfVectorizer(\n",
    "            lowercase=True, \n",
    "            analyzer='word', \n",
    "            min_df=min_df, \n",
    "            ngram_range=ngram_range,\n",
    "            stop_words=stop_words,\n",
    "            max_features=max_features\n",
    "        ), 'body')\n",
    "        ],\n",
    "        remainder=\"passthrough\"\n",
    "    )\n",
    "    \n",
    "    # Train/Validation Split\n",
    "    X_sub_train, X_sub_valid, y_sub_train, y_sub_valid = train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=0.2,\n",
    "        random_state=42 + trial.number,\n",
    "        stratify=y_train\n",
    "    )\n",
    "    vectorizer.fit(X_sub_train)\n",
    "    X_sub_train_vec = vectorizer.transform(X_sub_train)\n",
    "    X_sub_valid_vec = vectorizer.transform(X_sub_valid)\n",
    "    \n",
    "    # Model with pruning + early stopping\n",
    "    pruning_callback = XGBoostPruningCallback(trial, \"validation_0-auc\")\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        n_jobs=-1,\n",
    "        tree_method='hist',\n",
    "        device='cuda' if GPU else 'cpu',\n",
    "        random_state=42,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=30,\n",
    "        callbacks=[pruning_callback]\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_sub_train_vec,\n",
    "        y_sub_train,\n",
    "        eval_set=[(X_sub_valid_vec, y_sub_valid)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    preds = model.predict(X_sub_valid_vec)\n",
    "    f1 = f1_score(y_sub_valid, preds)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Trial {trial.number} done in {elapsed:.2f}s | F1: {f1:.4f}\")\n",
    "    return f1\n",
    "    \n",
    "start = time.perf_counter()\n",
    "study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner(n_startup_trials=20, n_warmup_steps=20, interval_steps=100))\n",
    "\n",
    "study.optimize(objective, n_trials=100, n_jobs=1)\n",
    "\n",
    "end = time.perf_counter()\n",
    "duration = end - start\n",
    "print(f\"Study took: {duration//(60**2)}m {duration//60}m {round(duration%60)}s\")\n",
    "print(f\"Best value: {study.best_value}\")\n",
    "print(f\"Best params: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0225d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5265b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
